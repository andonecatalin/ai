{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Execute once cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch.nn as nn\n",
    "#import data_download\n",
    "import functions\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import polars as pl\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname='ai_dataset'\n",
    "dbuser='postgres'\n",
    "dbpassword='parola'\n",
    "dbport=5432\n",
    "dbhost='127.0.0.1'\n",
    "table_name='tabela'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers=[\"AAPL\",\"MSFT\",'SPY','XAUUSD.OANDA','BCO.ICMTRADER']\n",
    "for i in range(len(tickers)):\n",
    "    tickers[i]=tickers[i].replace('.','_')\n",
    "net_functions=functions.Protected_execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4_530, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>aapl_open</th><th>aapl_high</th><th>aapl_low</th><th>aapl_close</th><th>aapl_adj_close</th><th>aapl_volume</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2.585</td><td>2.6696</td><td>2.5804</td><td>2.2722</td><td>2.2722</td><td>8.0723443e8</td></tr><tr><td>2.6832</td><td>2.7136</td><td>2.6607</td><td>2.2789</td><td>2.2789</td><td>6.196036e8</td></tr><tr><td>2.6725</td><td>2.675</td><td>2.6339</td><td>2.2609</td><td>2.2609</td><td>4.494224e8</td></tr><tr><td>2.6875</td><td>2.7393</td><td>2.6625</td><td>2.3193</td><td>2.3193</td><td>7.044576e8</td></tr><tr><td>2.7404</td><td>2.7571</td><td>2.705</td><td>2.3118</td><td>2.3118</td><td>6.7504077e8</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>195.18</td><td>195.41</td><td>192.97</td><td>193.6</td><td>193.6</td><td>3.71228e7</td></tr><tr><td>193.61</td><td>193.89</td><td>192.83</td><td>193.05</td><td>193.05</td><td>2.89193e7</td></tr><tr><td>192.49</td><td>193.5</td><td>191.09</td><td>193.15</td><td>193.15</td><td>4.80877e7</td></tr><tr><td>194.14</td><td>194.66</td><td>193.17</td><td>193.58</td><td>193.58</td><td>3.40499e7</td></tr><tr><td>193.9</td><td>194.4</td><td>191.73</td><td>192.53</td><td>192.53</td><td>4.26288e7</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4_530, 6)\n",
       "┌───────────┬───────────┬──────────┬────────────┬────────────────┬─────────────┐\n",
       "│ aapl_open ┆ aapl_high ┆ aapl_low ┆ aapl_close ┆ aapl_adj_close ┆ aapl_volume │\n",
       "│ ---       ┆ ---       ┆ ---      ┆ ---        ┆ ---            ┆ ---         │\n",
       "│ f64       ┆ f64       ┆ f64      ┆ f64        ┆ f64            ┆ f64         │\n",
       "╞═══════════╪═══════════╪══════════╪════════════╪════════════════╪═════════════╡\n",
       "│ 2.585     ┆ 2.6696    ┆ 2.5804   ┆ 2.2722     ┆ 2.2722         ┆ 8.0723443e8 │\n",
       "│ 2.6832    ┆ 2.7136    ┆ 2.6607   ┆ 2.2789     ┆ 2.2789         ┆ 6.196036e8  │\n",
       "│ 2.6725    ┆ 2.675     ┆ 2.6339   ┆ 2.2609     ┆ 2.2609         ┆ 4.494224e8  │\n",
       "│ 2.6875    ┆ 2.7393    ┆ 2.6625   ┆ 2.3193     ┆ 2.3193         ┆ 7.044576e8  │\n",
       "│ 2.7404    ┆ 2.7571    ┆ 2.705    ┆ 2.3118     ┆ 2.3118         ┆ 6.7504077e8 │\n",
       "│ …         ┆ …         ┆ …        ┆ …          ┆ …              ┆ …           │\n",
       "│ 195.18    ┆ 195.41    ┆ 192.97   ┆ 193.6      ┆ 193.6          ┆ 3.71228e7   │\n",
       "│ 193.61    ┆ 193.89    ┆ 192.83   ┆ 193.05     ┆ 193.05         ┆ 2.89193e7   │\n",
       "│ 192.49    ┆ 193.5     ┆ 191.09   ┆ 193.15     ┆ 193.15         ┆ 4.80877e7   │\n",
       "│ 194.14    ┆ 194.66    ┆ 193.17   ┆ 193.58     ┆ 193.58         ┆ 3.40499e7   │\n",
       "│ 193.9     ┆ 194.4     ┆ 191.73   ┆ 192.53     ┆ 192.53         ┆ 4.26288e7   │\n",
       "└───────────┴───────────┴──────────┴────────────┴────────────────┴─────────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=net_functions.create_query(tickers[0],table_name)\n",
    "cursor,conn=net_functions.create_cursor(dbname,dbuser,dbpassword,dbport,dbhost)\n",
    "data=pl.read_database(query=query,connection=conn)\n",
    "#data=data.with_columns(pl.col('aapl_adj_close').pct_change().alias('labels'))\n",
    "#normalize with pandas\n",
    "#data= data.select((pl.all()-pl.all().mean()) / pl.all().std())\n",
    "\n",
    "data=data.drop_nulls()\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from polars\n",
    "#labels=data['labels']\n",
    "train_data=data['aapl_high']\n",
    "#make polars data into torch tensors\n",
    "train_data=torch.tensor(train_data)\n",
    "\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shorten the tensor\n",
    "shortened_tensor=net_functions.tensor_shortner(train_data, batch_size)\n",
    "#create batches of 32\n",
    "image_tensor=net_functions.image_builder(shortened_tensor,batch_size=batch_size)\n",
    "#create tensor\n",
    "labels=net_functions.change(image_tensor, batch_size)\n",
    "\n",
    "#cut the last image from data variable to adjust for change function\n",
    "image_tensor=image_tensor[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make labels and data into tensors\n",
    "labels=torch.tensor(labels,dtype=torch.int32)\n",
    "image_tensor=torch.tensor(image_tensor)\n",
    "\n",
    "#move them to gpu\n",
    "labels=labels.to(device)\n",
    "image_tensor=image_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data with sklearn\n",
    "train_dataset,test_dataset,train_labels,test_labels=train_test_split(image_tensor,labels,test_size=.1)\n",
    "\n",
    "#create pythorch datasets\n",
    "train_dataset=torch.utils.data.TensorDataset(train_dataset,train_labels)\n",
    "test_dataset=torch.utils.data.TensorDataset(test_dataset,test_labels)\n",
    "batchsize=32\n",
    "train_loader=DataLoader(train_dataset,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "#test_loader=DataLoader(test_data,batch_size=test_data.tensors[0].shape[0],drop_last=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=batchsize,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def createTheMNISTNet(printtoggle=False):\n",
    "\n",
    "  class mnistNet(nn.Module):\n",
    "    def __init__(self, printtoggle=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input layer for 1D data of size 32\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=2)\n",
    "        \n",
    "        \n",
    "        self.pool=nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # fully-connected layers\n",
    "        self.fc1 = nn.Linear(512, 256) \n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)  # Assuming 10 output classes\n",
    "        self.fc5 = nn.Linear(128, 10)  # Assuming 10 output classes\n",
    "\n",
    "        # toggle for printing out tensor sizes during forward prop\n",
    "        #self.print = printtoggle\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        # Use GPU if available\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.to(self.device)  # Move the model to the chosen device\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        # Move input to device\n",
    "        x = x.to(self.device)\n",
    "\n",
    "        # Add a singleton dimension for the channel axis\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "\n",
    "        # Convolution -> relu\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Convolution -> relu\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Flatten the feature map\n",
    "        x = x.view(-1, 256*2)\n",
    "        #print(f'Flattened: {x.shape}') if self.print else None\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # create the model instance\n",
    "  net = mnistNet(printtoggle)\n",
    "  \n",
    "  # loss function (assuming classification task)\n",
    "  lossfun = nn.CrossEntropyLoss()\n",
    "\n",
    "  # optimizer\n",
    "  optimizer = torch.optim.Adam(net.parameters(),lr=.01)\n",
    "\n",
    "  return net,lossfun,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2trainTheModel(numepochs = 50):\n",
    "\n",
    "  # number of epochs\n",
    "  numepochs = 1000\n",
    "  \n",
    "  # create a new model\n",
    "  net, lossfun, optimizer = createTheMNISTNet()\n",
    "\n",
    "  # initialize losses\n",
    "  losses = torch.zeros(numepochs).to(device)\n",
    "  trainAcc = []\n",
    "  testAcc = []\n",
    "  i=0\n",
    "  z=0\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "    z+=1\n",
    "    # initialize batch losses and accuracies\n",
    "    epochLoss = 0.0\n",
    "    epochAcc = 0.0\n",
    "    print(z)\n",
    "    # loop over training data batches\n",
    "    for X, y in train_loader:\n",
    "      i+=1\n",
    "      #print(i)\n",
    "      X = X.unsqueeze(1)  # Add a singleton dimension for the channel axis\n",
    "     \n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat, y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # accumulate batch loss\n",
    "      epochLoss += loss.item()\n",
    "\n",
    "      # compute accuracy\n",
    "      matches = torch.argmax(yHat, axis=1) == y\n",
    "      accuracyPct = 100 * torch.mean(matches.float())\n",
    "      epochAcc += accuracyPct.item()\n",
    "\n",
    "    # end of batch loop\n",
    "\n",
    "    # average loss and accuracy for the epoch\n",
    "    epochLoss /= len(train_loader)\n",
    "    epochAcc /= len(train_loader)\n",
    "\n",
    "    # append to lists\n",
    "    losses[epochi] = epochLoss\n",
    "    trainAcc.append(epochAcc)\n",
    "\n",
    "    # test accuracy (evaluate on test set)\n",
    "    test_epochAcc = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    '''with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)  # Move data to device\n",
    "            yHat_test = net(X_test)\n",
    "            matches_test = torch.argmax(yHat_test, dim=1) == y_test\n",
    "            total_correct += matches_test.sum().item()\n",
    "            total_samples += y_test.size(0)\n",
    "    \n",
    "    test_epochAcc = 100 * total_correct / total_samples\n",
    "    testAcc.append(test_epochAcc)\n",
    "\n",
    "     ''' \n",
    "    #test_epochAcc = 0.0\n",
    "    #with torch.no_grad():\n",
    "      #for X_test, y_test in test_loader:\n",
    "        #yHat_test = net(X_test)\n",
    "        #matches_test = torch.argmax(yHat_test, axis=1) == y_test\n",
    "        #accuracyPct_test = 100 * torch.mean(matches_test.float())\n",
    "        #test_epochAcc += accuracyPct_test.item()\n",
    "    #test_epochAcc /= len(test_loader)\n",
    "    #testAcc.append(test_epochAcc)\n",
    "\n",
    "  # end epochs loop\n",
    "\n",
    "  # function output\n",
    "  return trainAcc, testAcc, losses, net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [32, 1, 1, 1, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainAcc,testAcc,losses,net \u001b[38;5;241m=\u001b[39m \u001b[43mfunction2trainTheModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m, in \u001b[0;36mfunction2trainTheModel\u001b[0;34m(numepochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add a singleton dimension for the channel axis\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# forward pass and loss\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m yHat \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m lossfun(yHat, y)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# backprop\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m, in \u001b[0;36mcreateTheMNISTNet.<locals>.mnistNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Convolution -> relu\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [32, 1, 1, 1, 32]"
     ]
    }
   ],
   "source": [
    "\n",
    "trainAcc,testAcc,losses,net = function2trainTheModel(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nploss=losses.to(\"cpu\")\n",
    "nploss=nploss.cpu()\n",
    "nploss=nploss.cpu().numpy()\n",
    "plt.plot(nploss)\n",
    "plt.title('losses over time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, testloader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data=data.unsqueeze(1)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = 100. * correct / len(testloader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(testloader.dataset)} '\n",
    "          f'({accuracy:.0f}%)\\n')\n",
    "    print(nploss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(net,device,test_loader,nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
