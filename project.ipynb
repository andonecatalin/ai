{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "start_date=\"2018-01-01\"\n",
    "end_date=\"2023-12-31\"\n",
    "tickers=[\"AAPL\",\"^GSPC\",\"MSFT\"]\n",
    "data1=yf.download('AAPL',start=start_date,end=end_date)\n",
    "data2=yf.download('^GSPC',start=start_date,end=end_date)\n",
    "data3=yf.download('MSFT',start=start_date,end=end_date)\n",
    "\n",
    "#for i in range(len(tickers)):\n",
    "#    df=pd.concat([df,yf.download(tickers[i],start=start_date,end=end_date)],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.rename(columns={'Open':'AAPL_Open'},inplace=True)\n",
    "data1.rename(columns={'High':'AAPL_High'},inplace=True)\n",
    "data1.rename(columns={'Low':'AAPL_Low'},inplace=True)\n",
    "data1.rename(columns={'Close':'AAPL_Close'},inplace=True)\n",
    "data1.rename(columns={'Adj Close':'AAPL_Adj Close'},inplace=True)\n",
    "data1.rename(columns={'Volume':'AAPL_Volume'},inplace=True)\n",
    "\n",
    "data2.rename(columns={'Open':'^GSPC_Open'},inplace=True)\n",
    "data2.rename(columns={'High':'^GSPC_High'},inplace=True)\n",
    "data2.rename(columns={'Low':'^GSPC_Low'},inplace=True)\n",
    "data2.rename(columns={'Close':'^GSPC_Close'},inplace=True)\n",
    "data2.rename(columns={'Adj Close':'^GSPC_Adj Close'},inplace=True)\n",
    "data2.rename(columns={'Volume':'^GSPC_Volume'},inplace=True)\n",
    "\n",
    "data3.rename(columns={'Open':'MSFT_Open'},inplace=True)\n",
    "data3.rename(columns={'High':'MSFT_High'},inplace=True)\n",
    "data3.rename(columns={'Low':'MSFT_Low'},inplace=True)\n",
    "data3.rename(columns={'Close':'MSFT_Close'},inplace=True)\n",
    "data3.rename(columns={'Adj Close':'MSFT_Adj Close'},inplace=True)\n",
    "data3.rename(columns={'Volume':'MSFT_Volume'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL_Open</th>\n",
       "      <th>AAPL_High</th>\n",
       "      <th>AAPL_Low</th>\n",
       "      <th>AAPL_Close</th>\n",
       "      <th>AAPL_Adj Close</th>\n",
       "      <th>AAPL_Volume</th>\n",
       "      <th>^GSPC_Open</th>\n",
       "      <th>^GSPC_High</th>\n",
       "      <th>^GSPC_Low</th>\n",
       "      <th>^GSPC_Close</th>\n",
       "      <th>^GSPC_Adj Close</th>\n",
       "      <th>^GSPC_Volume</th>\n",
       "      <th>MSFT_Open</th>\n",
       "      <th>MSFT_High</th>\n",
       "      <th>MSFT_Low</th>\n",
       "      <th>MSFT_Close</th>\n",
       "      <th>MSFT_Adj Close</th>\n",
       "      <th>MSFT_Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>42.540001</td>\n",
       "      <td>43.075001</td>\n",
       "      <td>42.314999</td>\n",
       "      <td>43.064999</td>\n",
       "      <td>40.670971</td>\n",
       "      <td>102223600</td>\n",
       "      <td>2683.729980</td>\n",
       "      <td>2695.889893</td>\n",
       "      <td>2682.360107</td>\n",
       "      <td>2695.810059</td>\n",
       "      <td>2695.810059</td>\n",
       "      <td>3397430000</td>\n",
       "      <td>86.129997</td>\n",
       "      <td>86.309998</td>\n",
       "      <td>85.500000</td>\n",
       "      <td>85.949997</td>\n",
       "      <td>80.080925</td>\n",
       "      <td>22483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>43.132500</td>\n",
       "      <td>43.637501</td>\n",
       "      <td>42.990002</td>\n",
       "      <td>43.057499</td>\n",
       "      <td>40.663898</td>\n",
       "      <td>118071600</td>\n",
       "      <td>2697.850098</td>\n",
       "      <td>2714.370117</td>\n",
       "      <td>2697.770020</td>\n",
       "      <td>2713.060059</td>\n",
       "      <td>2713.060059</td>\n",
       "      <td>3544030000</td>\n",
       "      <td>86.059998</td>\n",
       "      <td>86.510002</td>\n",
       "      <td>85.970001</td>\n",
       "      <td>86.349998</td>\n",
       "      <td>80.453606</td>\n",
       "      <td>26061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>43.134998</td>\n",
       "      <td>43.367500</td>\n",
       "      <td>43.020000</td>\n",
       "      <td>43.257500</td>\n",
       "      <td>40.852772</td>\n",
       "      <td>89738400</td>\n",
       "      <td>2719.310059</td>\n",
       "      <td>2729.290039</td>\n",
       "      <td>2719.070068</td>\n",
       "      <td>2723.989990</td>\n",
       "      <td>2723.989990</td>\n",
       "      <td>3697340000</td>\n",
       "      <td>86.589996</td>\n",
       "      <td>87.660004</td>\n",
       "      <td>86.570000</td>\n",
       "      <td>87.110001</td>\n",
       "      <td>81.161728</td>\n",
       "      <td>21912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>43.360001</td>\n",
       "      <td>43.842499</td>\n",
       "      <td>43.262501</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>41.317898</td>\n",
       "      <td>94640000</td>\n",
       "      <td>2731.330078</td>\n",
       "      <td>2743.449951</td>\n",
       "      <td>2727.919922</td>\n",
       "      <td>2743.149902</td>\n",
       "      <td>2743.149902</td>\n",
       "      <td>3239280000</td>\n",
       "      <td>87.660004</td>\n",
       "      <td>88.410004</td>\n",
       "      <td>87.430000</td>\n",
       "      <td>88.190002</td>\n",
       "      <td>82.167953</td>\n",
       "      <td>23407100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>43.587502</td>\n",
       "      <td>43.902500</td>\n",
       "      <td>43.482498</td>\n",
       "      <td>43.587502</td>\n",
       "      <td>41.164429</td>\n",
       "      <td>82271200</td>\n",
       "      <td>2742.669922</td>\n",
       "      <td>2748.510010</td>\n",
       "      <td>2737.600098</td>\n",
       "      <td>2747.709961</td>\n",
       "      <td>2747.709961</td>\n",
       "      <td>3246160000</td>\n",
       "      <td>88.199997</td>\n",
       "      <td>88.580002</td>\n",
       "      <td>87.599998</td>\n",
       "      <td>88.279999</td>\n",
       "      <td>82.251816</td>\n",
       "      <td>22113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-22</th>\n",
       "      <td>195.179993</td>\n",
       "      <td>195.410004</td>\n",
       "      <td>192.970001</td>\n",
       "      <td>193.600006</td>\n",
       "      <td>193.353287</td>\n",
       "      <td>37122800</td>\n",
       "      <td>4753.919922</td>\n",
       "      <td>4772.939941</td>\n",
       "      <td>4736.770020</td>\n",
       "      <td>4754.629883</td>\n",
       "      <td>4754.629883</td>\n",
       "      <td>3046770000</td>\n",
       "      <td>373.679993</td>\n",
       "      <td>375.179993</td>\n",
       "      <td>372.709991</td>\n",
       "      <td>374.579987</td>\n",
       "      <td>373.888580</td>\n",
       "      <td>17091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>193.610001</td>\n",
       "      <td>193.889999</td>\n",
       "      <td>192.830002</td>\n",
       "      <td>193.050003</td>\n",
       "      <td>192.803986</td>\n",
       "      <td>28919300</td>\n",
       "      <td>4758.859863</td>\n",
       "      <td>4784.720215</td>\n",
       "      <td>4758.450195</td>\n",
       "      <td>4774.750000</td>\n",
       "      <td>4774.750000</td>\n",
       "      <td>2513910000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>376.940002</td>\n",
       "      <td>373.500000</td>\n",
       "      <td>374.660004</td>\n",
       "      <td>373.968445</td>\n",
       "      <td>12673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>192.490005</td>\n",
       "      <td>193.500000</td>\n",
       "      <td>191.089996</td>\n",
       "      <td>193.149994</td>\n",
       "      <td>192.903839</td>\n",
       "      <td>48087700</td>\n",
       "      <td>4773.450195</td>\n",
       "      <td>4785.390137</td>\n",
       "      <td>4768.899902</td>\n",
       "      <td>4781.580078</td>\n",
       "      <td>4781.580078</td>\n",
       "      <td>2748450000</td>\n",
       "      <td>373.690002</td>\n",
       "      <td>375.059998</td>\n",
       "      <td>372.809998</td>\n",
       "      <td>374.070007</td>\n",
       "      <td>373.379547</td>\n",
       "      <td>14905400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>194.139999</td>\n",
       "      <td>194.660004</td>\n",
       "      <td>193.169998</td>\n",
       "      <td>193.580002</td>\n",
       "      <td>193.333298</td>\n",
       "      <td>34049900</td>\n",
       "      <td>4786.439941</td>\n",
       "      <td>4793.299805</td>\n",
       "      <td>4780.979980</td>\n",
       "      <td>4783.350098</td>\n",
       "      <td>4783.350098</td>\n",
       "      <td>2698860000</td>\n",
       "      <td>375.369995</td>\n",
       "      <td>376.459991</td>\n",
       "      <td>374.160004</td>\n",
       "      <td>375.279999</td>\n",
       "      <td>374.587280</td>\n",
       "      <td>14327000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>193.899994</td>\n",
       "      <td>194.399994</td>\n",
       "      <td>191.729996</td>\n",
       "      <td>192.529999</td>\n",
       "      <td>192.284637</td>\n",
       "      <td>42628800</td>\n",
       "      <td>4782.879883</td>\n",
       "      <td>4788.430176</td>\n",
       "      <td>4751.990234</td>\n",
       "      <td>4769.830078</td>\n",
       "      <td>4769.830078</td>\n",
       "      <td>3126060000</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>377.160004</td>\n",
       "      <td>373.480011</td>\n",
       "      <td>376.040009</td>\n",
       "      <td>375.345886</td>\n",
       "      <td>18723000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1509 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             AAPL_Open   AAPL_High    AAPL_Low  AAPL_Close  AAPL_Adj Close  \\\n",
       "Date                                                                         \n",
       "2018-01-02   42.540001   43.075001   42.314999   43.064999       40.670971   \n",
       "2018-01-03   43.132500   43.637501   42.990002   43.057499       40.663898   \n",
       "2018-01-04   43.134998   43.367500   43.020000   43.257500       40.852772   \n",
       "2018-01-05   43.360001   43.842499   43.262501   43.750000       41.317898   \n",
       "2018-01-08   43.587502   43.902500   43.482498   43.587502       41.164429   \n",
       "...                ...         ...         ...         ...             ...   \n",
       "2023-12-22  195.179993  195.410004  192.970001  193.600006      193.353287   \n",
       "2023-12-26  193.610001  193.889999  192.830002  193.050003      192.803986   \n",
       "2023-12-27  192.490005  193.500000  191.089996  193.149994      192.903839   \n",
       "2023-12-28  194.139999  194.660004  193.169998  193.580002      193.333298   \n",
       "2023-12-29  193.899994  194.399994  191.729996  192.529999      192.284637   \n",
       "\n",
       "            AAPL_Volume   ^GSPC_Open   ^GSPC_High    ^GSPC_Low  ^GSPC_Close  \\\n",
       "Date                                                                          \n",
       "2018-01-02    102223600  2683.729980  2695.889893  2682.360107  2695.810059   \n",
       "2018-01-03    118071600  2697.850098  2714.370117  2697.770020  2713.060059   \n",
       "2018-01-04     89738400  2719.310059  2729.290039  2719.070068  2723.989990   \n",
       "2018-01-05     94640000  2731.330078  2743.449951  2727.919922  2743.149902   \n",
       "2018-01-08     82271200  2742.669922  2748.510010  2737.600098  2747.709961   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2023-12-22     37122800  4753.919922  4772.939941  4736.770020  4754.629883   \n",
       "2023-12-26     28919300  4758.859863  4784.720215  4758.450195  4774.750000   \n",
       "2023-12-27     48087700  4773.450195  4785.390137  4768.899902  4781.580078   \n",
       "2023-12-28     34049900  4786.439941  4793.299805  4780.979980  4783.350098   \n",
       "2023-12-29     42628800  4782.879883  4788.430176  4751.990234  4769.830078   \n",
       "\n",
       "            ^GSPC_Adj Close  ^GSPC_Volume   MSFT_Open   MSFT_High    MSFT_Low  \\\n",
       "Date                                                                            \n",
       "2018-01-02      2695.810059    3397430000   86.129997   86.309998   85.500000   \n",
       "2018-01-03      2713.060059    3544030000   86.059998   86.510002   85.970001   \n",
       "2018-01-04      2723.989990    3697340000   86.589996   87.660004   86.570000   \n",
       "2018-01-05      2743.149902    3239280000   87.660004   88.410004   87.430000   \n",
       "2018-01-08      2747.709961    3246160000   88.199997   88.580002   87.599998   \n",
       "...                     ...           ...         ...         ...         ...   \n",
       "2023-12-22      4754.629883    3046770000  373.679993  375.179993  372.709991   \n",
       "2023-12-26      4774.750000    2513910000  375.000000  376.940002  373.500000   \n",
       "2023-12-27      4781.580078    2748450000  373.690002  375.059998  372.809998   \n",
       "2023-12-28      4783.350098    2698860000  375.369995  376.459991  374.160004   \n",
       "2023-12-29      4769.830078    3126060000  376.000000  377.160004  373.480011   \n",
       "\n",
       "            MSFT_Close  MSFT_Adj Close  MSFT_Volume  \n",
       "Date                                                 \n",
       "2018-01-02   85.949997       80.080925     22483800  \n",
       "2018-01-03   86.349998       80.453606     26061400  \n",
       "2018-01-04   87.110001       81.161728     21912000  \n",
       "2018-01-05   88.190002       82.167953     23407100  \n",
       "2018-01-08   88.279999       82.251816     22113000  \n",
       "...                ...             ...          ...  \n",
       "2023-12-22  374.579987      373.888580     17091100  \n",
       "2023-12-26  374.660004      373.968445     12673100  \n",
       "2023-12-27  374.070007      373.379547     14905400  \n",
       "2023-12-28  375.279999      374.587280     14327000  \n",
       "2023-12-29  376.040009      375.345886     18723000  \n",
       "\n",
       "[1509 rows x 18 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.merge(data1,data2,on='Date',how='right')\n",
    "data=pd.merge(data,data3,on='Date',how='right')\n",
    "data.to_csv('AAPL,MSFT,SPX.csv')\n",
    "data_corection=torch.as_tensor(data.values,dtype=torch.float32)\n",
    "data_corection=data_corection.to(device)\n",
    "\n",
    "data.to_csv('short_stocks.csv')\n",
    "\n",
    "data_row=data_corection.t()\n",
    "data_row.to(device)\n",
    "data_row.size()\n",
    "data_corection.size()\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AI(nn.Module):\n",
    "    def __init__(self,nUnits,nLayers):\n",
    "        super(AI,self).__init__()\n",
    "        \n",
    "        # create dictionary to store the layers\n",
    "        self.layers = nn.ModuleDict()\n",
    "        self.nLayers = nLayers\n",
    "\n",
    "        ### input layer\n",
    "        self.layers['input'] = nn.Linear(18,1509)\n",
    "        self.layers['hidden0']=nn.Linear(1509,nUnits)\n",
    "        \n",
    "        ### hidden layers\n",
    "        for i in range(nLayers):\n",
    "            self.layers[f'hidden{i+1}'] = nn.Linear(nUnits,nUnits)\n",
    "\n",
    "        ### output layer\n",
    "        self.layers['output'] = nn.Linear(nUnits,1)\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.layers['input'](x))\n",
    "        for i in range(self.nLayers):\n",
    "            x=F.relu(self.layers[f'hidden{i}'](x))\n",
    "        x=self.layers['output'](x)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nUnitsPerLayer=512\n",
    "nLayers=100\n",
    "net=AI(1509,nLayers)\n",
    "net=net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 43.6375,  43.3675,  43.8425,  ..., 193.5000, 194.6600, 194.4000],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=data['AAPL_High']\n",
    "target=target[1:]\n",
    "target=torch.tensor(target.values)\n",
    "target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTheModel(theModel,numepochs):\n",
    "    lossfun=nn.SmoothL1Loss()\n",
    "    optimizer=torch.optim.RMSprop(theModel.parameters(),lr=.01,momentum=.9)\n",
    "    losses=torch.zeros(numepochs).to(device)\n",
    "    for epochi in range(numepochs):\n",
    "        print(epochi)\n",
    "        yHat=theModel(data_row)\n",
    "        loss=lossfun(yHat,target)\n",
    "        losses[epochi]=loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (18x1509 and 18x1509)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses\u001b[38;5;241m=\u001b[39m\u001b[43mtrainTheModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnumepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses\u001b[38;5;241m.\u001b[39mdetach(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m,markerfacecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m,linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.1\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[42], line 7\u001b[0m, in \u001b[0;36mtrainTheModel\u001b[0;34m(theModel, numepochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epochi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numepochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(epochi)\n\u001b[0;32m----> 7\u001b[0m     yHat\u001b[38;5;241m=\u001b[39m\u001b[43mtheModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_row\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss\u001b[38;5;241m=\u001b[39mlossfun(yHat,target)\n\u001b[1;32m      9\u001b[0m     losses[epochi]\u001b[38;5;241m=\u001b[39mloss\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 20\u001b[0m, in \u001b[0;36mAI.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 20\u001b[0m     x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnLayers):\n\u001b[1;32m     22\u001b[0m         x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m](x))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (18x1509 and 18x1509)"
     ]
    }
   ],
   "source": [
    "numepochs=10000\n",
    "losses=trainTheModel(net,numepochs)\n",
    "plt.plot(losses.detach(),'o',markerfacecolor='w',linewidth=.1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
