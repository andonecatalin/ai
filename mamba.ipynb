{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andrei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-03-10 20:52:40,166 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-03-10 20:52:40,167 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n",
      "c:\\Users\\andrei\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"VisionMambaBlock module.\"\"\"\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import nn, Tensor\n",
    "from zeta.nn import SSM\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import filecmp\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from sklearn import preprocessing as pre\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_head(dim: int, num_classes: int):\n",
    "    \"\"\"\n",
    "    Creates a head for the output layer of a model.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input dimension of the head.\n",
    "        num_classes (int): The number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        nn.Sequential: The output head module.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.LayerNorm(dim),\n",
    "        nn.Linear(dim, num_classes),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionEncoderMambaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    VisionMambaBlock is a module that implements the Mamba block from the paper\n",
    "    Vision Mamba: Efficient Visual Representation Learning with Bidirectional\n",
    "    State Space Model\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input dimension of the input tensor.\n",
    "        heads (int): The number of heads in the multi-head attention mechanism.\n",
    "        dt_rank (int): The rank of the state space model.\n",
    "        dim_inner (int): The dimension of the inner layer of the\n",
    "            multi-head attention.\n",
    "        d_state (int): The dimension of the state space model.\n",
    "\n",
    "\n",
    "    Example:\n",
    "    >>> block = VisionMambaBlock(dim=256, heads=8, dt_rank=32,\n",
    "            dim_inner=512, d_state=256)\n",
    "    >>> x = torch.randn(1, 32, 256)\n",
    "    >>> out = block(x)\n",
    "    >>> out.shape\n",
    "    torch.Size([1, 32, 256])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        heads: int,\n",
    "        dt_rank: int,\n",
    "        dim_inner: int,\n",
    "        d_state: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.dt_rank = dt_rank\n",
    "        self.dim_inner = dim_inner\n",
    "        self.d_state = d_state\n",
    "\n",
    "        self.forward_conv1d = nn.Conv1d(\n",
    "            in_channels=dim, out_channels=dim, kernel_size=1\n",
    "        )\n",
    "        self.backward_conv1d = nn.Conv1d(\n",
    "            in_channels=dim, out_channels=dim, kernel_size=1\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.ssm = SSM(dim, dt_rank, dim_inner, d_state)\n",
    "\n",
    "        # Linear layer for z and x\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        # Softplus\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x is of shape [batch_size, seq_len, dim]\n",
    "        b, s, d = x.shape\n",
    "\n",
    "        # Skip connection\n",
    "        skip = x\n",
    "\n",
    "        # Normalization\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Split x into x1 and x2 with linears\n",
    "        z1 = self.proj(x)\n",
    "        x1 = self.proj(x)\n",
    "\n",
    "        # forward con1d\n",
    "        x1_rearranged = rearrange(x1, \"b s d -> b d s\")\n",
    "        forward_conv_output = self.forward_conv1d(x1_rearranged)\n",
    "        forward_conv_output = rearrange(\n",
    "            forward_conv_output, \"b d s -> b s d\"\n",
    "        )\n",
    "        x1_ssm = self.ssm(forward_conv_output)\n",
    "\n",
    "        # backward conv x2\n",
    "        x2_rearranged = rearrange(x1, \"b s d -> b d s\")\n",
    "        x2 = self.backward_conv1d(x2_rearranged)\n",
    "        x2 = rearrange(x2, \"b d s -> b s d\")\n",
    "\n",
    "        # Backward ssm\n",
    "        x2 = self.ssm(x2)\n",
    "\n",
    "        # Activation\n",
    "        z = self.activation(z1)\n",
    "\n",
    "        # matmul with z + backward ssm\n",
    "        x2 = x2 @ z\n",
    "\n",
    "        # Matmul with z and x1\n",
    "        x1 = x1_ssm @ z\n",
    "\n",
    "        # Add both matmuls\n",
    "        x = x1 + x2\n",
    "\n",
    "        # Add skip connection\n",
    "        return x + skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        heads: int = 8,\n",
    "        dt_rank: int = 32,\n",
    "        dim_inner: int = None,\n",
    "        d_state: int = None,\n",
    "        num_classes: int = None,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        channels: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "        depth: int = 12,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.dt_rank = dt_rank\n",
    "        self.dim_inner = dim_inner\n",
    "        self.d_state = d_state\n",
    "        self.num_classes = num_classes\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.channels = channels\n",
    "        self.dropout = dropout\n",
    "        self.depth = depth\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange(\n",
    "                \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\",\n",
    "                p1=patch_height,\n",
    "                p2=patch_height,\n",
    "            ),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        # Latent\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        # encoder layers\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Append the encoder layers\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                VisionEncoderMambaBlock(\n",
    "                    dim=dim,\n",
    "                    heads=heads,\n",
    "                    dt_rank=dt_rank,\n",
    "                    dim_inner=dim_inner,\n",
    "                    d_state=d_state,\n",
    "                    *args,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output head\n",
    "        self.output_head = output_head(dim, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # Patch embedding\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x = self.to_patch_embedding(x)\n",
    "        print(f\"Patch embedding: {x.shape}\")\n",
    "\n",
    "        # Shape\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Cls tokens\n",
    "        cls_tokens = repeat(self.cls_token, \"() n d -> b n d\", b=b)\n",
    "        print(f\"Cls tokens: {cls_tokens.shape}\")\n",
    "\n",
    "        # Concatenate\n",
    "        # x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        # Forward pass with the layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            print(f\"Layer: {x.shape}\")\n",
    "\n",
    "        # Latent\n",
    "        x = self.to_latent(x)\n",
    "\n",
    "        # Output head with the cls tokens\n",
    "        return self.output_head(x)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
